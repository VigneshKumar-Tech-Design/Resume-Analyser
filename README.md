# Resume Radar - AI-Powered Resume Evaluation System

![frontend/assets/logo-lightmode.png](frontend/assets/logo-lightmode.png) ![frontend/assets/logo-nightmode.png](frontend/assets/logo-nightmode.png)

**Resume Radar** is an advanced AI-based resume evaluation tool that leverages the locally-installed **Ollama3** LLM to provide comprehensive resume analysis and feedback. This system offers both administrator and standard user interfaces for seamless resume evaluation.

---

## ğŸš€ Features

- ğŸ¤– AI-powered resume analysis using **LLaMA 3** via **Ollama**
- ğŸ” Local processing ensures **data privacy**
- ğŸ‘¤ Dual interface: **Administrator** and **Standard User**
- â±ï¸ Real-time processing metrics
- ğŸŒ Responsive and user-friendly web interface

---

## ğŸ’» System Requirements

- Windows 10/11 or Linux
- Python 3.9+
- PowerShell 7+
- NVIDIA GPU (recommended) with CUDA support
- 8GB+ RAM (16GB recommended)
- Ollama3 model installed locally

---

## âš™ï¸ Installation

### 1. Clone the Repository

```bash

git clone https://github.com/yourusername/Resume_Radar.git
cd Resume_Radar

```

### 2. Set Up Python Environment

```powershell

# Create virtual environment
python -m venv venv

# Activate environment
.\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

```
### 3. Install Ollama (One-Time Setup)

ğŸ”§ Windows

1. Download the installer from the official Ollama site:
ğŸ‘‰ https://ollama.com/download

2. Run the installer and follow the setup instructions.

3. After installation, verify it in PowerShell:

``` powershell

ollama --version # Check Ollama's Version - You should see the installed version of Ollama.

```
ğŸ§ Linux (Debian/Ubuntu)

```bash

curl -fsSL https://ollama.com/install.sh | sh

```
Then verify:

```bash

ollama --version

```

ğŸ’¡ You might need to restart your shell or system after installation for changes to take effect.

### 4. Install the LLAMA 3 Model

Once Ollama is installed, pull the required model:

```powershell

ollama pull llama3 

```

Then Verify:

```

ollama list

```

## ğŸ“ Project Structure

```bash

Resume_Radar/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ venv/                # Python virtual environment
â”‚   â”œâ”€â”€ uploads/             # Resume storage directory
â”‚   â”œâ”€â”€ .env                 # Environment configuration
â”‚   â”œâ”€â”€ app.py               # Flask application
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ logo-lightmode.png
â”‚   â”‚   â””â”€â”€ logo-nightmode.png
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css        # Frontend styles
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ script.js        # Frontend logic
â”‚   â””â”€â”€ index.html           # Main interface
â””â”€â”€ README.md                # This documentation

```

## ğŸ‘¨â€ğŸ’» Usage

### Administrator Mode

1. Open PowerShell as Administrator

2. Navigate to the project directory:

```powershell

cd "E:\Documents\Website and portfolio\resume\backend"

```

3. Activate the virtual environment:

```powershell

.\venv\Scripts\activate

```

4. Start the Flask application:

```powershell

python app.py

```

### Standard User Mode

1. Open PowerShell

2. Use the following commands:

```powershell

ollama list               # Check available models
ollama pull llama3        # Ensure model is available
ollama serve              # Start model server

```

## ğŸ“¡ API Endpoints

POST /api/analyze â€“ Submit resume for analysis

POST /api/generate â€“ Generate evaluation report


## ğŸ› ï¸ Troubleshooting

### Model not loading:

1. Verify Ollama3 is properly installed

2. Check GPU drivers and CUDA setup

3. Ensure sufficient system resources

### API timeouts:

1. Increase timeout limits in frontend configuration

2. Ensure Ollama server is running

## ğŸ“„ License

This project is licensed under the MIT License. See the LICENSE file for details.

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what youâ€™d like to change.

## ğŸ™ Acknowledgments

Ollama for the LLM framework

Flask for backend services


